import re
import os
from collections import Counter
from os import listdir
from os.path import isfile, join
import os,sys,argparse
import pytorch_pretrained_bert
from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertModel, BertConfig
from pytorch_pretrained_bert import BertTokenizer
from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE
import torch
from torch import nn
from torch.nn import CrossEntropyLoss
import torch.optim as optim
import numpy as np
import torch.nn.functional as F
import random
import read_brat
import calc_coref_metrics

import pytorch_pretrained_bert
from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertModel
from pytorch_pretrained_bert import BertTokenizer
from name_coref import NameCoref

from torch.optim.lr_scheduler import ExponentialLR

random.seed(1)
np.random.seed(1)
torch.manual_seed(1)

bert_dim=768
HIDDEN_DIM=200

nameCoref=NameCoref()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
cpu=torch.device("cpu")
		

def get_matrix(list_of_entities, max_words, max_ents):
	matrix=np.zeros((max_ents, max_words))
	for idx, (start, end) in enumerate(list_of_entities):
		for i in range(start, end+1):
			matrix[idx,i]=1
	return matrix


def get_mention_width_bucket(dist):
	if dist < 10:
		return dist + 1

	return 11

def get_distance_bucket(dist):
	if dist < 5:
		return dist+1

	elif dist >= 5 and dist <= 7:
		return 6
	elif dist >= 8 and dist <= 15:
		return 7
	elif dist >= 16 and dist <= 31:
		return 8
	elif dist >= 32 and dist <= 63:
		return 9

	return 10

vec_get_distance_bucket=np.vectorize(get_distance_bucket)

def get_nested(start1, end1, start2, end2):
	if start1 <= start2 and end2 <= end1:
		return 1
	return 0

male_terms={"his":1, "him":1, "he":1}
female_terms={"her":1, "she":1}

pronouns={"she", "he", "him", "her", "his", "it"}

def read_dicts(maleTerms, femaleTerms):
	with open(maleTerms, encoding="utf-8") as file:
		for line in file:
			phrase=line.rstrip().lower()
			male_terms[phrase]=1

	with open(femaleTerms, encoding="utf-8") as file:
		for line in file:
			phrase=line.rstrip().lower()
			female_terms[phrase]=1


def get_inquote(start, end, sent):

	inQuote=False
	quotes=[]

	for token in sent:
		if token == "â€œ" or token == "\"":
			if inQuote == True:
				inQuote=False
			else:
				inQuote=True

		quotes.append(inQuote)

	for i in range(start, end+1):
		if quotes[i] == True:
			return 1

	return 0

def get_gender(phrase):
	phrase=phrase.lower()
	final=phrase.split(" ")[-1]
	if phrase.startswith("mrs") or final in female_terms:
		return 1
	if phrase.startswith("mr.") or phrase.startswith("mr ") or final in male_terms:
		return 2
	return 0


class LSTMTagger(BertPreTrainedModel):

	def __init__(self, config, freeze_bert=False):
		super(LSTMTagger, self).__init__(config)

		hidden_dim=HIDDEN_DIM
		self.hidden_dim=hidden_dim

		self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False, do_basic_tokenize=False)
		self.bert = BertModel.from_pretrained("bert-base-cased")
		self.bert.eval()

		if freeze_bert:
			for param in self.bert.parameters():
				param.requires_grad = False

		self.distance_embeddings = nn.Embedding(11, 20)
		self.sent_distance_embeddings = nn.Embedding(11, 20)
		self.nested_embeddings = nn.Embedding(2, 20)
		self.gender_embeddings = nn.Embedding(3, 20)
		self.width_embeddings = nn.Embedding(12, 20)
		self.quote_embeddings = nn.Embedding(3, 20)

		self.lstm = nn.LSTM(4*bert_dim, hidden_dim, bidirectional=True, batch_first=True)

		self.attention1 = nn.Linear(hidden_dim * 2, hidden_dim * 2)
		self.attention2 = nn.Linear(hidden_dim * 2, 1)
		self.mention_mention1 = nn.Linear( (3 * 2 * hidden_dim + 20 + 20) * 3 + 20 + 20 + 20 + 20, 150)
		self.mention_mention2 = nn.Linear(150, 150)
		self.mention_mention3 = nn.Linear(150, 1)

		self.unary1 = nn.Linear(3 * 2 * hidden_dim + 20 + 20, 150)
		self.unary2 = nn.Linear(150, 150)
		self.unary3 = nn.Linear(150, 1)

		self.drop_layer_050 = nn.Dropout(p=0.5)
		self.drop_layer_020 = nn.Dropout(p=0.2)
		self.tanh = nn.Tanh()
		self.relu = nn.ReLU()

		self.apply(self.init_bert_weights)


	def get_mention_reps(self, input_ids=None, attention_mask=None, starts=None, ends=None, index=None, widths=None, genders=None, quotes=None, matrix=None, transforms=None, doTrain=True):

		starts=starts.to(device)
		ends=ends.to(device)
		widths=widths.to(device)
		genders=genders.to(device)
		quotes=quotes.to(device)

		input_ids = input_ids.to(device)
		attention_mask = attention_mask.to(device)
		transforms = transforms.to(device)

		# matrix specifies which token positions (cols) are associated with which mention spans (row)
		matrix=matrix.to(device) # num_sents x max_ents x max_words

		# index specifies the location of the mentions in each sentence (which vary due to padding)
		index=index.to(device)

		sequence_outputs, pooled_outputs = self.bert(input_ids, token_type_ids=None, attention_mask=attention_mask)
		all_layers = torch.cat((sequence_outputs[-1], sequence_outputs[-2], sequence_outputs[-3], sequence_outputs[-4]), 2)
		embeds=torch.matmul(transforms,all_layers)
		lstm_output, _ = self.lstm(embeds) # num_sents x max_words x 2 * hidden_dim

		###########
		# ATTENTION OVER MENTION
		###########

		attention_weights=self.attention2(self.tanh(self.attention1(lstm_output))) # num_sents x max_words x 1
		attention_weights=torch.exp(attention_weights)
		
		attx=attention_weights.squeeze(-1).unsqueeze(1).expand_as(matrix)
		summer=attx*matrix

		val=matrix*summer # num_sents x max_ents x max_words
		
		val=val/torch.sum(1e-8+val,dim=2).unsqueeze(-1)

		attended=torch.matmul(val, lstm_output) # num_sents x max_ents x 2 * hidden_dim

		attended=attended.view(-1,2*self.hidden_dim)

		lstm_output=lstm_output.contiguous()
		position_output=lstm_output.view(-1, 2*self.hidden_dim)
		
		# starts = token position of beginning of mention in flattened token list
		start_output=torch.index_select(position_output, 0, starts)
		# ends = token position of end of mention in flattened token list
		end_output=torch.index_select(position_output, 0, ends)

		# index = index of entity in flattened list of attended mention representations
		mentions=torch.index_select(attended, 0, index)

		width_embeds=self.width_embeddings(widths)
		gender_embeds=self.gender_embeddings(genders)
		quote_embeds=self.quote_embeddings(quotes)

		span_representation=torch.cat((start_output, end_output, mentions, width_embeds, quote_embeds), 1)

		if doTrain:
			return span_representation
		else:
			# detach tensor from computation graph or memory will blow up
			return span_representation.detach()

	def forward(self, matrix, index, truth=None, existing=None, names=None, max_words=100, max_ents=100, token_positions=None, starts=None, ends=None, widths=None, input_ids=None, attention_mask=None, transforms=None, genders=None, quotes=None):


		entity_properties={}
		all_gends=[]
		for gen in genders:
			all_gends.extend(gen.data)


		doTrain=False
		if truth is not None:
			doTrain=True

		zeroTensor=torch.FloatTensor([0]).to(device)

		all_starts=None
		all_ends=None

		span_representation=None

		all_all=[]
		for b in range(len(matrix)):

			span_reps=self.get_mention_reps(input_ids=input_ids[b], attention_mask=attention_mask[b], starts=starts[b], ends=ends[b], index=index[b], widths=widths[b], genders=genders[b], quotes=quotes[b], transforms=transforms[b], matrix=matrix[b], doTrain=doTrain)
			if b == 0:
				span_representation=span_reps
				all_starts=starts[b]
				all_ends=ends[b]

			else:

				span_representation=torch.cat((span_representation, span_reps), 0)
	
				all_starts=torch.cat((all_starts, starts[b]), 0)
				all_ends=torch.cat((all_ends, ends[b]), 0)

		all_starts=all_starts.to(device)
		all_ends=all_ends.to(device)
		
		num_mentions,=all_starts.shape

		running_loss=0

		curid=-1

		if existing is not None:
			for r in existing:
				if r > curid:
					curid=r
		curid+=1

		assignments=[]

		seen={}

		ch=0

		token_positions=np.array(token_positions)

		mention_index=np.arange(num_mentions)

		unary_scores=self.unary3(self.tanh(self.drop_layer_020(self.unary2(self.tanh(self.drop_layer_020(self.unary1(span_representation)))))))

		for i in range(num_mentions):

			if i == 0:
				# the first mention must start a new entity; this doesn't affect training (since the loss must be 0) so we can skip it.
				if truth is None:

					if existing is None or (existing is not None and existing[i] == -1):
						assignment=curid
						curid+=1
						assignments.append(assignment)
					else:
						assignments.append(existing[i])
				
				continue

			MAX_PREVIOUS_MENTIONS=300

			first=0
			if truth is None:
				if len(names[i]) == 1 and names[i][0].lower() in pronouns:
					MAX_PREVIOUS_MENTIONS=20

				first=i-MAX_PREVIOUS_MENTIONS
				if first < 0:
					first=0

			targets=span_representation[first:i]
			cp=span_representation[i].expand_as(targets)
			
			dists=[]
			nesteds=[]

			# get distance in mentions
			distances=i-mention_index[first:i]
			dists=vec_get_distance_bucket(distances)
			dists=torch.LongTensor(dists).to(device)
			distance_embeds=self.distance_embeddings(dists)

			# get distance in sentences
			sent_distances=token_positions[i]-token_positions[first:i]
			sent_dists=vec_get_distance_bucket(sent_distances)
			sent_dists=torch.LongTensor(sent_dists).to(device)
			sent_distance_embeds=self.sent_distance_embeddings(sent_dists)

			# is the current mention nested within a previous one?
			res1=(all_starts[first:i] <= all_starts[i]) 
			res2=(all_ends[i] <= all_ends[first:i])

			nesteds=(res1*res2).long()
			nesteds_embeds=self.nested_embeddings(nesteds)

			res1=(all_starts[i] <= all_starts[first:i]) 
			res2=(all_ends[first:i] <= all_ends[i])

			nesteds=(res1*res2).long()
			nesteds_embeds2=self.nested_embeddings(nesteds)

			elementwise=cp*targets
			concat=torch.cat((cp, targets, elementwise, distance_embeds, sent_distance_embeds, nesteds_embeds, nesteds_embeds2), 1)

			preds=self.mention_mention3(self.tanh(self.drop_layer_020(self.mention_mention2(self.tanh(self.drop_layer_020(self.mention_mention1(concat)))))))

			preds=preds + unary_scores[i] + unary_scores[first:i]

			preds=preds.squeeze(-1)

			if truth is not None:
	
				# zero is the score for the dummy antecedent/new entity
				preds=torch.cat((preds, zeroTensor))
	
				golds_sum=0.
				preds_sum=torch.logsumexp(preds, 0)

				if len(truth[i]) == 1 and truth[i][-1] not in seen:
					golds_sum=0.
					seen[truth[i][-1]]=1
				else:
					golds=torch.index_select(preds, 0, torch.LongTensor(truth[i]).to(device))
					golds_sum=torch.logsumexp(golds, 0)
				
				# want to maximize (golds_sum-preds_sum), so minimize (preds_sum-golds_sum)
				diff=preds_sum-golds_sum

				running_loss+=diff

			else:

				if existing is None or (existing is not None and existing[i] == -1):

					assignment=None

					if i == 0:
						assignment=curid
						curid+=1

					else:

						arg_sorts=torch.argsort(preds, descending=True)
						k=0
						while k < len(arg_sorts):
							if k > 0:
								print("skipping,", k)
							cand_idx=arg_sorts[k]
							if preds[cand_idx] > 0:
								cand_assignment=assignments[cand_idx+first]
								assignment=cand_assignment
								ch+=1
								break

							else:
								assignment=curid
								curid+=1
								break

							k+=1

					assignments.append(assignment)
			
				else:
					assignments.append(existing[i])

		if truth is not None:
			return running_loss
		else:
			print("Antecedents: %s/%s" % (ch, num_mentions))
			return assignments

	def print_conll(self, name, sents, all_ents, assignments, out):

		doc_id, part_id=name

		mapper=[]
		idd=0
		for ent in all_ents:
			mapper_e=[]
			for e in ent:
				mapper_e.append(idd)
				idd+=1
			mapper.append(mapper_e)

		out.write("#begin document (%s); part %s\n" % (doc_id, part_id))
		
		for s_idx, sent in enumerate(sents):
			ents=all_ents[s_idx]
			for w_idx, word in enumerate(sent):
				if w_idx == 0 or w_idx == len(sent)-1:
					continue

				label=[]
				for idx, (start, end) in enumerate(ents):

					if start == w_idx and end == w_idx:
						eid=assignments[mapper[s_idx][idx]]
						label.append("(%s)" % eid)
					elif start == w_idx:
						eid=assignments[mapper[s_idx][idx]]
						label.append("(%s" % eid)
					elif end == w_idx:
						eid=assignments[mapper[s_idx][idx]]
						label.append("%s)" % eid)

				out.write("%s\t%s\t%s\t%s\t_\t_\t_\t_\t_\t_\t_\t_\t%s\n" % (doc_id, part_id, w_idx-1, word, '|'.join(label)))

			if len(sent) > 2:
				out.write("\n")

		out.write("#end document\n")


	def test(self, test_all_docs, test_all_ents, test_all_named_ents, test_all_max_words, test_all_max_ents, test_doc_names, outfile, iterr, goldFile, doTest=False):

		out=open(outfile, "w", encoding="utf-8")

		# for each document
		for idx in range(len(test_all_docs)):

			d,p=test_doc_names[idx]
			d=re.sub("/", "_", d)
			test_doc=test_all_docs[idx]
			test_ents=test_all_ents[idx]

			max_words=test_all_max_words[idx]
			max_ents=test_all_max_ents[idx]

			names=[]
			for n_idx, sent in enumerate(test_ents):
				for ent in sent:
					name=test_doc[n_idx][ent[0]:ent[1]+1]
					names.append(name)


			named_index={}
			for sidx, sentence in enumerate(test_all_named_ents[idx]):
				for start, end, _ in sentence:
					named_index[(sidx, start, end)]=1

			is_named=[]

			for sidx, sentence in enumerate(test_all_ents[idx]):
				for (start, end) in sentence:
					if (sidx, start, end) in named_index:
						is_named.append(1)
					else:
						is_named.append(0)

			refs=None
			refs=nameCoref.name_cluster(names, is_named)

			test_matrix, test_index, test_token_positions, test_ent_spans, test_starts, test_ends, test_widths, test_data, test_masks, test_transforms, test_genders, test_quotes=self.get_data(test_doc, test_ents, max_ents, max_words)

			assignments=self.forward(test_matrix, test_index, names=names, existing=refs, token_positions=test_token_positions, starts=test_starts, ends=test_ends, widths=test_widths, input_ids=test_data, attention_mask=test_masks, transforms=test_transforms, genders=test_genders, quotes=test_quotes)
			self.print_conll(test_doc_names[idx], test_doc, test_ents, assignments, out)

		out.close()

		if doTest:
			print("Goldfile: %s" % goldFile)
			print("Predfile: %s" % outfile)
			
			bcub_f, avg=calc_coref_metrics.get_conll(gold=goldFile, preds=outfile)
			print("Iter %s, Average F1: %.3f, bcub F1: %s" % (iterr, avg, bcub_f))
			sys.stdout.flush()
			return avg


	def get_data(self, doc, ents, max_ents, max_words):

		batchsize=128

		token_positions=[]
		ent_spans=[]
		genders=[]
		persons=[]
		inquotes=[]

		batch_matrix=[]
		matrix=[]

		max_words_batch=[]
		max_ents_batch=[]

		max_w=1
		max_e=1

		sent_count=0
		for idx, sent in enumerate(doc):
			
			if len(sent) > max_w:
				max_w=len(sent)
			if len(ents[idx]) > max_e:
				max_e = len(ents[idx])

			sent_count+=1

			if sent_count == batchsize:
				max_words_batch.append(max_w)
				max_ents_batch.append(max_e)
				sent_count=0
				max_w=0
				max_e=0

		if sent_count > 0:
			max_words_batch.append(max_w)
			max_ents_batch.append(max_e)

		batch_count=0
		for idx, sent in enumerate(doc):
			matrix.append(get_matrix(ents[idx], max_words_batch[batch_count], max_ents_batch[batch_count]))

			if len(matrix) == batchsize:
				batch_matrix.append(torch.FloatTensor(matrix))
				matrix=[]
				batch_count+=1

		if len(matrix) > 0:
			batch_matrix.append(torch.FloatTensor(matrix))


		batch_index=[]
		batch_genders=[]
		batch_quotes=[]

		batch_ent_spans=[]

		index=[]
		abs_pos=0
		sent_count=0

		b=0
		for idx, sent in enumerate(ents):

			for i in range(len(sent)):
				index.append(sent_count*max_ents_batch[b] + i)
				s,e=sent[i]
				token_positions.append(idx)
				ent_spans.append(e-s)
				phrase=' '.join(doc[idx][s:e+1])

				genders.append(get_gender(phrase))
				inquotes.append(get_inquote(s, e, doc[idx]))


			abs_pos+=len(doc[idx])

			sent_count+=1

			if sent_count == batchsize:
				batch_index.append(torch.LongTensor(index))
				batch_genders.append(torch.LongTensor(genders))
				batch_quotes.append(torch.LongTensor(inquotes))
				batch_ent_spans.append(ent_spans)

				index=[]
				genders=[]
				inquotes=[]
				ent_spans=[]
				sent_count=0
				b+=1

		if sent_count > 0:
			batch_index.append(torch.LongTensor(index))
			batch_genders.append(torch.LongTensor(genders))
			batch_quotes.append(torch.LongTensor(inquotes))
			batch_ent_spans.append(ent_spans)

		all_masks=[]
		all_transforms=[]
		all_data=[]

		batch_masks=[]
		batch_transforms=[]
		batch_data=[]

		# get ids and pad sentence
		for sent in doc:
			tok_ids=[]
			input_mask=[]
			transform=[]

			all_toks=[]
			n=0
			for idx, word in enumerate(sent):
				toks=self.tokenizer.tokenize(word)
				all_toks.append(toks)
				n+=len(toks)


			cur=0
			for idx, word in enumerate(sent):

				toks=all_toks[idx]
				ind=list(np.zeros(n))
				for j in range(cur,cur+len(toks)):
					ind[j]=1./len(toks)
				cur+=len(toks)
				transform.append(ind)

				tok_id=self.tokenizer.convert_tokens_to_ids(toks)
				assert len(tok_id) == len(toks)
				tok_ids.extend(tok_id)

				input_mask.extend(np.ones(len(toks)))

				token=word.lower()

			all_masks.append(input_mask)
			all_data.append(tok_ids)
			all_transforms.append(transform)

			if len(all_masks) == batchsize:
				batch_masks.append(all_masks)
				batch_data.append(all_data)
				batch_transforms.append(all_transforms)

				all_masks=[]
				all_data=[]
				all_transforms=[]

		if len(all_masks) > 0:
			batch_masks.append(all_masks)
			batch_data.append(all_data)
			batch_transforms.append(all_transforms)


		for b in range(len(batch_data)):

			max_len = max([len(sent) for sent in batch_data[b]])

			for j in range(len(batch_data[b])):
				
				blen=len(batch_data[b][j])

				for k in range(blen, max_len):
					batch_data[b][j].append(0)
					batch_masks[b][j].append(0)
					for z in range(len(batch_transforms[b][j])):
						batch_transforms[b][j][z].append(0)

				for k in range(len(batch_transforms[b][j]), max_words_batch[b]):
					batch_transforms[b][j].append(np.zeros(max_len))

			batch_data[b]=torch.LongTensor(batch_data[b])
			batch_transforms[b]=torch.FloatTensor(batch_transforms[b])
			batch_masks[b]=torch.FloatTensor(batch_masks[b])
			
		tok_pos=0
		starts=[]
		ends=[]
		widths=[]

		batch_starts=[]
		batch_ends=[]
		batch_widths=[]

		sent_count=0
		b=0
		for idx, sent in enumerate(ents):

			for i in range(len(sent)):

				s,e=sent[i]

				starts.append(tok_pos+s)
				ends.append(tok_pos+e)
				widths.append(get_mention_width_bucket(e-s))

			sent_count+=1
			tok_pos+=max_words_batch[b]

			if sent_count == batchsize:
				batch_starts.append(torch.LongTensor(starts))
				batch_ends.append(torch.LongTensor(ends))
				batch_widths.append(torch.LongTensor(widths))

				starts=[]
				ends=[]
				widths=[]
				tok_pos=0
				sent_count=0
				b+=1

		if sent_count > 0:
			batch_starts.append(torch.LongTensor(starts))
			batch_ends.append(torch.LongTensor(ends))
			batch_widths.append(torch.LongTensor(widths))


		return batch_matrix, batch_index, token_positions, ent_spans, batch_starts, batch_ends, batch_widths, batch_data, batch_masks, batch_transforms, batch_genders, batch_quotes


def get_ant_labels(all_doc_sents, all_doc_ents):

	max_words=0
	max_ents=0
	mention_id=0

	big_ents={}

	doc_antecedent_labels=[]
	big_doc_ents=[]


	for idx, sent in enumerate(all_doc_sents):
		if len(sent) > max_words:
			max_words=len(sent)

		this_sent_ents=[]

		all_sent_ents=sorted(all_doc_ents[idx], key=lambda x: (x[0], x[1]))
		if len(all_sent_ents) > max_ents:
			max_ents=len(all_sent_ents)

		for (w_idx_start, w_idx_end, eid) in all_sent_ents:

			this_sent_ents.append((w_idx_start, w_idx_end))

			coref={}
			if eid in big_ents:
				coref=big_ents[eid]
			else:
				coref={mention_id:1}

			vals=sorted(coref.keys())

			if eid not in big_ents:
				big_ents[eid]={}

			big_ents[eid][mention_id]=1
			mention_id+=1

			doc_antecedent_labels.append(vals)

		big_doc_ents.append(this_sent_ents)


	return doc_antecedent_labels, big_doc_ents, max_words, max_ents


def read_conll(filename, model=None):

	docid=None
	partID=None

	# collection
	all_sents=[]
	all_ents=[]
	all_antecedent_labels=[]
	all_max_words=[]
	all_max_ents=[]
	all_doc_names=[]

	all_named_ents=[]


	# for one doc
	all_doc_sents=[]
	all_doc_ents=[]
	all_doc_named_ents=[]

	# for one sentence
	sent=[]
	ents=[]
	sent.append("[SEP]")
	sid=0

	named_ents=[]
	cur_tokens=0
	max_allowable_tokens=400
	cur_tid=0
	open_count=0
	with open(filename, encoding="utf-8") as file:
		for line in file:
			if line.startswith("#begin document"):

				all_doc_ents=[]
				all_doc_sents=[]

				all_doc_named_ents=[]

				open_ents={}
				open_named_ents={}

				sid=0
				docid=None

				matcher=re.match("#begin document \((.*)\); part (.*)$", line.rstrip())
				if matcher != None:
					docid=matcher.group(1)
					partID=matcher.group(2)

				print(docid)

			elif line.startswith("#end document"):

				all_sents.append(all_doc_sents)
				
				doc_antecedent_labels, big_ents, max_words, max_ents=get_ant_labels(all_doc_sents, all_doc_ents)

				all_ents.append(big_ents)

				all_named_ents.append(all_doc_named_ents)

				all_antecedent_labels.append(doc_antecedent_labels)
				all_max_words.append(max_words+1)
				all_max_ents.append(max_ents+1)
				
				all_doc_names.append((docid,partID))

			else:

				parts=re.split("\s+", line.rstrip())

				if len(parts) < 2 or (cur_tokens >= max_allowable_tokens and open_count == 0):
		
					sent.append("[CLS]")
					all_doc_sents.append(sent)
					ents=sorted(ents, key=lambda x: (x[0], x[1]))

					all_doc_ents.append(ents)

					all_doc_named_ents.append(named_ents)

					ents=[]
					named_ents=[]
					sent=[]
					sent.append("[SEP]")
					sid+=1

					cur_tokens=0

					cur_tid=0

					if len(parts) < 2:
						continue

				tid=cur_tid+1
				token=parts[3]
				coref=parts[-1].split("|")
				b_toks=model.tokenizer.tokenize(token)
				cur_tokens+=len(b_toks)
				cur_tid+=1

				for c in coref:
					if c.startswith("(") and c.endswith(")"):
						c=re.sub("\(", "", c)
						c=int(re.sub("\)", "", c))

						ents.append((tid, tid, c))

					elif c.startswith("("):
						c=int(re.sub("\(", "", c))

						if c not in open_ents:
							open_ents[c]=[]
						open_ents[c].append(tid)
						open_count+=1

					elif c.endswith(")"):
						c=int(re.sub("\)", "", c))

						assert c in open_ents

						start_tid=open_ents[c].pop()
						open_count-=1

						ents.append((start_tid, tid, c))

				ner=parts[10].split("|")

				for c in ner:
					try:
						if c.startswith("(") and c.endswith(")"):
							c=re.sub("\(", "", c)
							c=int(re.sub("\)", "", c))

							named_ents.append((tid, tid, c))

						elif c.startswith("("):
							c=int(re.sub("\(", "", c))

							if c not in open_named_ents:
								open_named_ents[c]=[]
							open_named_ents[c].append(tid)

						elif c.endswith(")"):
							c=int(re.sub("\)", "", c))

							assert c in open_named_ents

							start_tid=open_named_ents[c].pop()

							named_ents.append((start_tid, tid, c))
					except:
						pass

				sent.append(token)

	return all_sents, all_ents, all_named_ents, all_antecedent_labels, all_max_words, all_max_ents, all_doc_names

def read_dir(filename):

	all_docs=[]
	all_ents=[]
	all_truth=[]
	all_max_words=[]
	all_max_ents=[]
	
	doc_ids=[]

	onlyfiles = [f for f in listdir(filename) if isfile(join(filename, f))]
	for filen in onlyfiles:
		if filen.endswith(".ann"):
			base=re.sub(".ann$", "", filen)
			doc_ids.append((base, "0"))
			print(base)
			ents=read_brat.read_coref(os.path.join(filename, "%s.out" % base))
			ents=read_brat.read_anns(os.path.join(filename, "%s.ann" % base), ents)
			doc, ents, truth, max_words, max_ents=read_brat.read_txt(os.path.join(filename, "%s.txt" % base), ents)

			all_docs.append(doc)
			all_ents.append(ents)
			all_truth.append(truth)
			all_max_words.append(max_words)
			all_max_ents.append(max_ents)

	return all_docs, all_ents, all_truth, all_max_words, all_max_ents, doc_ids

if __name__ == "__main__":

	parser = argparse.ArgumentParser()
	parser.add_argument('-t','--trainData', help='Folder containing train data', required=False)
	parser.add_argument('-v','--valData', help='Folder containing test data', required=False)
	parser.add_argument('-m','--mode', help='mode {train, predict}', required=False)
	parser.add_argument('-w','--model', help='modelFile (to write to or read from)', required=False)
	parser.add_argument('-o','--outFile', help='outFile', required=False)

	args = vars(parser.parse_args())

	mode=args["mode"]
	modelFile=args["model"]
	valData=args["valData"]
	outfile=args["outFile"]
	
	cache_dir = os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(0))

	model = LSTMTagger.from_pretrained('bert-base-cased',
			  cache_dir=cache_dir,
			  freeze_bert=True)

	config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,
		num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)

	model.to(device)

	optimizer = optim.Adam(model.parameters(), lr=0.001)
	lr_scheduler=ExponentialLR(optimizer, gamma=0.999)

	if mode == "train":

		trainData=args["trainData"]

		all_docs, all_ents, all_named_ents, all_truth, all_max_words, all_max_ents, doc_ids=read_conll(trainData, model)
		test_all_docs, test_all_ents, test_all_named_ents, test_all_truth, test_all_max_words, test_all_max_ents, test_doc_ids=read_conll(valData, model)


		best_f1=0.
		cur_steps=0

		best_idx=0
		patience=10

		for i in range(100):

			model.train()
			bigloss=0.
			for idx in range(len(all_docs)):

				if idx % 10 == 0:
					print(idx, "/", len(all_docs))
					sys.stdout.flush()
				max_words=all_max_words[idx]
				max_ents=all_max_ents[idx]

				matrix, index, token_positions, ent_spans, starts, ends, widths, input_ids, masks, transforms, genders, quotes=model.get_data(all_docs[idx], all_ents[idx], max_ents, max_words)

				if max_ents > 1:
					model.zero_grad()

					loss=model.forward(matrix, index, truth=all_truth[idx], max_words=max_words, max_ents=max_ents, names=None, token_positions=token_positions, starts=starts, ends=ends, widths=widths, input_ids=input_ids, attention_mask=masks, transforms=transforms, genders=genders, quotes=quotes)
					loss.backward()
					optimizer.step()
					cur_steps+=1
					if cur_steps % 100 == 0:
						lr_scheduler.step()
				bigloss+=loss.item()

			print(bigloss)

			model.eval()
			doTest=False
			if i >= 2:
				doTest=True
			
			avg_f1=model.test(test_all_docs, test_all_ents, test_all_named_ents, test_all_max_words, test_all_max_ents, test_doc_ids, outfile, i, valData, doTest=doTest)

			if doTest:
				if avg_f1 > best_f1:
					torch.save(model.state_dict(), modelFile)
					print("Saving model ... %.3f is better than %.3f" % (avg_f1, best_f1))
					best_f1=avg_f1
					best_idx=i

				if i-best_idx > patience:
					print ("Stopping training at epoch %s" % epoch)
					break

	elif mode == "predict":

		model.load_state_dict(torch.load(modelFile, map_location=device))
		model.eval()
		test_all_docs, test_all_ents, test_all_named_ents, test_all_truth, test_all_max_words, test_all_max_ents, test_doc_ids=read_conll(valData, model=model)

		model.test(test_all_docs, test_all_ents, test_all_named_ents, test_all_max_words, test_all_max_ents, test_doc_ids, outfile, 0, valData, doTest=False)

